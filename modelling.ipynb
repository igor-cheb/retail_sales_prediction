{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall idea is that we will implement linear and tree based models first and try to ansamble them. Further will implement recurrent NN and compare the two models.\n",
    "Data normalisation required for linear or NN models will be done with the help of sklearn pipeline which will include both motel and data transofrmation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from src.ToyModel import ToyModel\n",
    "from src.utilities import run_cv\n",
    "from src.FeatureGenerator import FeatureGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV data initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating data generators\n",
    "feat_generator = FeatureGenerator()\n",
    "\n",
    "# initiating cv splitter\n",
    "tscv = TimeSeriesSplit(test_size = 1, max_train_size=16)\n",
    "\n",
    "# generating all of the data which we will iterate over during CV\n",
    "features_df = feat_generator.generate_features()\n",
    "\n",
    "# creating col lists for training\n",
    "cols_di={\n",
    "    'index': feat_generator.index_cols,\n",
    "    'target': feat_generator.target_col,\n",
    "    'feats': feat_generator.shifted_cols + feat_generator.roll_cols\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train months: [13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28], size: 120,494\n",
      "  Test months: [29],   size: 7,039\n",
      "  NRMSE:  1.0\n",
      "  RMSE :  5.1\n",
      "\n",
      "Fold 1:\n",
      "  Train months: [14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], size: 119,696\n",
      "  Test months: [30],   size: 6,739\n",
      "  NRMSE:  1.0\n",
      "  RMSE :  4.4\n",
      "\n",
      "Fold 2:\n",
      "  Train months: [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30], size: 118,557\n",
      "  Test months: [31],   size: 5,669\n",
      "  NRMSE:  1.0\n",
      "  RMSE :  4.4\n",
      "\n",
      "Fold 3:\n",
      "  Train months: [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31], size: 116,851\n",
      "  Test months: [32],   size: 5,439\n",
      "  NRMSE:  1.0\n",
      "  RMSE :  4.9\n",
      "\n",
      "Fold 4:\n",
      "  Train months: [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32], size: 114,617\n",
      "  Test months: [33],   size: 5,668\n",
      "  NRMSE:  1.0\n",
      "  RMSE :  3.9\n",
      "\n",
      "\n",
      "------------------------------\n",
      "RMSE mean: 4.6\n",
      "NRMSE mean: 1.0\n"
     ]
    }
   ],
   "source": [
    "# initiating toy model\n",
    "toy_model = ToyModel()\n",
    "\n",
    "# iterating over CV folds\n",
    "toy_cv_res = run_cv(df=features_df, months_cv_split=tscv, model=toy_model, cols_di=cols_di, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train months: [13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28], size: 120,494\n",
      "  Test months: [29],   size: 7,039\n",
      "  NRMSE:  0.5\n",
      "  RMSE :  2.6\n",
      "\n",
      "Fold 1:\n",
      "  Train months: [14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], size: 119,696\n",
      "  Test months: [30],   size: 6,739\n",
      "  NRMSE:  0.49\n",
      "  RMSE :  2.2\n",
      "\n",
      "Fold 2:\n",
      "  Train months: [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30], size: 118,557\n",
      "  Test months: [31],   size: 5,669\n",
      "  NRMSE:  0.4\n",
      "  RMSE :  1.7\n",
      "\n",
      "Fold 3:\n",
      "  Train months: [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31], size: 116,851\n",
      "  Test months: [32],   size: 5,439\n",
      "  NRMSE:  0.69\n",
      "  RMSE :  3.4\n",
      "\n",
      "Fold 4:\n",
      "  Train months: [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32], size: 114,617\n",
      "  Test months: [33],   size: 5,668\n",
      "  NRMSE:  0.71\n",
      "  RMSE :  2.7\n",
      "\n",
      "\n",
      "------------------------------\n",
      "RMSE mean: 2.5\n",
      "NRMSE mean: 0.56\n"
     ]
    }
   ],
   "source": [
    "# initiating lin model\n",
    "lin_model =  Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lin_model\", LinearRegression()) #ElasticNet() Lasso(alpha=.005) , \n",
    "])\n",
    "\n",
    "# iterating over CV folds\n",
    "lin_cv_res = run_cv(df=features_df, months_cv_split=tscv, model=lin_model, cols_di=cols_di, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.618315051373108"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tree_cv_res['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-20 11:11:30,575]\u001b[0m A new study created in memory with name: regression_2\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:14:30,365]\u001b[0m Trial 0 finished with value: 4.364637479027674 and parameters: {'max_depth': 5, 'learning_rate': 0.4409059972894563, 'n_estimators': 1310, 'gamma': 0.7157776884402807, 'reg_alpha': 1.8434339241821862, 'reg_lambda': 5.381226313674236, 'colsample_bytree': 0.9419783479713788, 'subsample': 0.5523431741830999}. Best is trial 0 with value: 4.364637479027674.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:17:40,549]\u001b[0m Trial 1 finished with value: 2.9820197331131983 and parameters: {'max_depth': 13, 'learning_rate': 0.072226991946282, 'n_estimators': 479, 'gamma': 0.16925556616877172, 'reg_alpha': 2.2345834594825926, 'reg_lambda': 3.7557035663949248, 'colsample_bytree': 0.950998681075538, 'subsample': 0.4780832571598659}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:19:05,904]\u001b[0m Trial 2 finished with value: 3.725726204167208 and parameters: {'max_depth': 9, 'learning_rate': 0.5824663571788544, 'n_estimators': 297, 'gamma': 0.46684286808791753, 'reg_alpha': 1.6841546297585288, 'reg_lambda': 6.615624421890996, 'colsample_bytree': 0.8595887917114841, 'subsample': 0.822499560874665}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:29:27,395]\u001b[0m Trial 3 finished with value: 6.416121126728936 and parameters: {'max_depth': 15, 'learning_rate': 0.4303762417106554, 'n_estimators': 1188, 'gamma': 0.24446728733312345, 'reg_alpha': 1.542409428505165, 'reg_lambda': 4.519247317803879, 'colsample_bytree': 0.651347611926961, 'subsample': 0.6475208596567207}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:35:41,190]\u001b[0m Trial 4 finished with value: 5.14876813549608 and parameters: {'max_depth': 9, 'learning_rate': 0.3790621948225714, 'n_estimators': 966, 'gamma': 0.1666393765112, 'reg_alpha': 3.6977862012255165, 'reg_lambda': 5.589950159240274, 'colsample_bytree': 0.9476819581658552, 'subsample': 0.7613013349373176}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:44:03,701]\u001b[0m Trial 5 finished with value: 3.4515713863168296 and parameters: {'max_depth': 10, 'learning_rate': 0.24293761031530522, 'n_estimators': 1411, 'gamma': 0.5246381948870217, 'reg_alpha': 4.563086891526589, 'reg_lambda': 2.1986664211012044, 'colsample_bytree': 0.859996630834003, 'subsample': 0.48516757491972223}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:49:27,683]\u001b[0m Trial 6 finished with value: 5.657192661862142 and parameters: {'max_depth': 7, 'learning_rate': 0.5120857528388789, 'n_estimators': 1366, 'gamma': 0.9594479820179893, 'reg_alpha': 4.087453132085595, 'reg_lambda': 1.3845285208214104, 'colsample_bytree': 0.8005840468498939, 'subsample': 0.6604158339711687}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:57:05,814]\u001b[0m Trial 7 finished with value: 4.298661092212577 and parameters: {'max_depth': 15, 'learning_rate': 0.435130511210149, 'n_estimators': 1010, 'gamma': 0.3079079048899338, 'reg_alpha': 1.4493949752037623, 'reg_lambda': 6.40541871930832, 'colsample_bytree': 0.5866333429465709, 'subsample': 0.49306668891268757}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 11:59:52,944]\u001b[0m Trial 8 finished with value: 3.076495334534105 and parameters: {'max_depth': 13, 'learning_rate': 0.03138860485320768, 'n_estimators': 337, 'gamma': 0.7518071386861901, 'reg_alpha': 2.591428812261523, 'reg_lambda': 5.679700173249113, 'colsample_bytree': 0.9126776100502545, 'subsample': 0.7814157796074115}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:01:52,820]\u001b[0m Trial 9 finished with value: 5.244658848729833 and parameters: {'max_depth': 5, 'learning_rate': 0.5234636451764398, 'n_estimators': 792, 'gamma': 0.8548465345064943, 'reg_alpha': 5.289565939720932, 'reg_lambda': 3.9032831958366767, 'colsample_bytree': 0.9020919426341362, 'subsample': 0.40827335734170644}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:02:20,539]\u001b[0m Trial 10 finished with value: 3.4719667559205356 and parameters: {'max_depth': 12, 'learning_rate': 0.005543671931999908, 'n_estimators': 93, 'gamma': 0.017607602119812354, 'reg_alpha': 6.575824473839187, 'reg_lambda': 0.2402843629401823, 'colsample_bytree': 0.43954694810047557, 'subsample': 0.9665006264969876}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:05:54,466]\u001b[0m Trial 11 finished with value: 3.842041787538856 and parameters: {'max_depth': 12, 'learning_rate': 0.020255153589521957, 'n_estimators': 466, 'gamma': 0.7190704014046585, 'reg_alpha': 0.19978731809633743, 'reg_lambda': 3.048323709572546, 'colsample_bytree': 0.9949510605337897, 'subsample': 0.8824458629000231}. Best is trial 1 with value: 2.9820197331131983.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:09:38,250]\u001b[0m Trial 12 finished with value: 2.9224217584275336 and parameters: {'max_depth': 13, 'learning_rate': 0.1390516981110381, 'n_estimators': 486, 'gamma': 0.5106847432255278, 'reg_alpha': 2.7710164075517305, 'reg_lambda': 4.680597259692162, 'colsample_bytree': 0.7652899016110178, 'subsample': 0.7316507005124847}. Best is trial 12 with value: 2.9224217584275336.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:14:31,295]\u001b[0m Trial 13 finished with value: 3.0917694908890203 and parameters: {'max_depth': 13, 'learning_rate': 0.18176749565119044, 'n_estimators': 633, 'gamma': 0.49248353387586513, 'reg_alpha': 3.004217859882082, 'reg_lambda': 3.3214298100032162, 'colsample_bytree': 0.737906967091293, 'subsample': 0.6052968815612625}. Best is trial 12 with value: 2.9224217584275336.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:18:55,341]\u001b[0m Trial 14 finished with value: 3.0873029213398873 and parameters: {'max_depth': 11, 'learning_rate': 0.13459685332982202, 'n_estimators': 620, 'gamma': 0.39322786181086855, 'reg_alpha': 0.10818886883898271, 'reg_lambda': 4.4864498808984274, 'colsample_bytree': 0.7538279706403943, 'subsample': 0.7193144301921838}. Best is trial 12 with value: 2.9224217584275336.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:19:37,579]\u001b[0m Trial 15 finished with value: 2.9008911546541176 and parameters: {'max_depth': 14, 'learning_rate': 0.1242062052576731, 'n_estimators': 122, 'gamma': 0.024101824215153334, 'reg_alpha': 2.652335980255277, 'reg_lambda': 2.543430155094484, 'colsample_bytree': 0.6224861945087364, 'subsample': 0.41480175325486834}. Best is trial 15 with value: 2.9008911546541176.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:19:41,542]\u001b[0m Trial 16 finished with value: 2.7755206047011898 and parameters: {'max_depth': 3, 'learning_rate': 0.3060334947078895, 'n_estimators': 44, 'gamma': 0.5966401997134406, 'reg_alpha': 5.076011013471811, 'reg_lambda': 2.3119294165592073, 'colsample_bytree': 0.5471362015597458, 'subsample': 0.8819402045252194}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:19:46,751]\u001b[0m Trial 17 finished with value: 2.829621529324977 and parameters: {'max_depth': 3, 'learning_rate': 0.29583805914163436, 'n_estimators': 65, 'gamma': 0.6134356690996401, 'reg_alpha': 5.393151832042293, 'reg_lambda': 2.0822360227306675, 'colsample_bytree': 0.5028929030875332, 'subsample': 0.9700973619372915}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:19:50,860]\u001b[0m Trial 18 finished with value: 3.2288527576263775 and parameters: {'max_depth': 3, 'learning_rate': 0.32412497058297884, 'n_estimators': 49, 'gamma': 0.619846814884058, 'reg_alpha': 5.921651079265384, 'reg_lambda': 1.2856249696969941, 'colsample_bytree': 0.510980712221114, 'subsample': 0.9988063418001377}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:20:08,763]\u001b[0m Trial 19 finished with value: 2.8428992394970076 and parameters: {'max_depth': 3, 'learning_rate': 0.2685890284934002, 'n_estimators': 254, 'gamma': 0.62266385425304, 'reg_alpha': 4.980600922984099, 'reg_lambda': 1.5979163565230243, 'colsample_bytree': 0.42224804154024287, 'subsample': 0.9105675282545299}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:20:35,515]\u001b[0m Trial 20 finished with value: 3.286381872975358 and parameters: {'max_depth': 5, 'learning_rate': 0.33059934996019863, 'n_estimators': 206, 'gamma': 0.8332302633741526, 'reg_alpha': 6.545721961449093, 'reg_lambda': 0.1307421828010833, 'colsample_bytree': 0.5396758909341833, 'subsample': 0.8712317305594133}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:20:53,951]\u001b[0m Trial 21 finished with value: 2.822379567397735 and parameters: {'max_depth': 3, 'learning_rate': 0.24751644076679188, 'n_estimators': 267, 'gamma': 0.614437307304882, 'reg_alpha': 5.057446927026913, 'reg_lambda': 1.70108500175182, 'colsample_bytree': 0.41101849945248636, 'subsample': 0.9219750721947314}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:21:11,534]\u001b[0m Trial 22 finished with value: 2.9854663539697497 and parameters: {'max_depth': 4, 'learning_rate': 0.2251077715035244, 'n_estimators': 184, 'gamma': 0.6001534107636511, 'reg_alpha': 5.668077250479972, 'reg_lambda': 2.194657760187166, 'colsample_bytree': 0.4813255407938713, 'subsample': 0.9377896642405785}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:21:18,168]\u001b[0m Trial 23 finished with value: 2.982705009766038 and parameters: {'max_depth': 7, 'learning_rate': 0.2913123886909156, 'n_estimators': 36, 'gamma': 0.646146799778143, 'reg_alpha': 4.381100079952298, 'reg_lambda': 0.9388277595535968, 'colsample_bytree': 0.5690815766100383, 'subsample': 0.8355880559660402}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:22:25,317]\u001b[0m Trial 24 finished with value: 2.785060634885604 and parameters: {'max_depth': 7, 'learning_rate': 0.20546146147681027, 'n_estimators': 396, 'gamma': 0.3769689773182321, 'reg_alpha': 6.999221965344249, 'reg_lambda': 2.730576235948039, 'colsample_bytree': 0.47786098281353695, 'subsample': 0.9544383395354883}. Best is trial 16 with value: 2.7755206047011898.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:23:21,536]\u001b[0m Trial 25 finished with value: 2.6593202056323966 and parameters: {'max_depth': 6, 'learning_rate': 0.1990326941083754, 'n_estimators': 408, 'gamma': 0.3996711215286497, 'reg_alpha': 6.143251558871656, 'reg_lambda': 2.9084725449137685, 'colsample_bytree': 0.4554708773680582, 'subsample': 0.9166094915804088}. Best is trial 25 with value: 2.6593202056323966.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:24:22,770]\u001b[0m Trial 26 finished with value: 2.645893200633254 and parameters: {'max_depth': 7, 'learning_rate': 0.19954706113792114, 'n_estimators': 362, 'gamma': 0.33508401169793434, 'reg_alpha': 6.092771756721105, 'reg_lambda': 2.8259388778880905, 'colsample_bytree': 0.46820045383805575, 'subsample': 0.8404918363404242}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:26:07,876]\u001b[0m Trial 27 finished with value: 2.853724530016119 and parameters: {'max_depth': 6, 'learning_rate': 0.16850135444282266, 'n_estimators': 639, 'gamma': 0.37888818258309376, 'reg_alpha': 6.089509235253952, 'reg_lambda': 2.9408562032874728, 'colsample_bytree': 0.6298675135736469, 'subsample': 0.8230107324580085}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:29:49,767]\u001b[0m Trial 28 finished with value: 3.9888758937367976 and parameters: {'max_depth': 8, 'learning_rate': 0.3597018034598406, 'n_estimators': 823, 'gamma': 0.29460016372980047, 'reg_alpha': 6.311665036802703, 'reg_lambda': 3.415337378617335, 'colsample_bytree': 0.6827778900245222, 'subsample': 0.8755025072006544}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:30:43,460]\u001b[0m Trial 29 finished with value: 2.9755570312094117 and parameters: {'max_depth': 6, 'learning_rate': 0.09799694038754705, 'n_estimators': 387, 'gamma': 0.4410413193692584, 'reg_alpha': 6.85350363107424, 'reg_lambda': 3.892619148441842, 'colsample_bytree': 0.45716758770985794, 'subsample': 0.779550516538143}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:31:36,281]\u001b[0m Trial 30 finished with value: 2.8778964007316032 and parameters: {'max_depth': 4, 'learning_rate': 0.19253456556087872, 'n_estimators': 543, 'gamma': 0.18067047137389103, 'reg_alpha': 5.699490079644571, 'reg_lambda': 0.8958003800910377, 'colsample_bytree': 0.548863241790094, 'subsample': 0.842840866310686}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:32:45,037]\u001b[0m Trial 31 finished with value: 2.8811951675303464 and parameters: {'max_depth': 7, 'learning_rate': 0.2123639630852051, 'n_estimators': 408, 'gamma': 0.3634091426724908, 'reg_alpha': 6.976749158190014, 'reg_lambda': 2.7422930034117354, 'colsample_bytree': 0.4713445140810222, 'subsample': 0.9490937629147769}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:35:10,001]\u001b[0m Trial 32 finished with value: 2.8829786662055286 and parameters: {'max_depth': 8, 'learning_rate': 0.07464052737763764, 'n_estimators': 706, 'gamma': 0.3027417785481096, 'reg_alpha': 6.259639398158216, 'reg_lambda': 2.413148753630999, 'colsample_bytree': 0.5195579970987521, 'subsample': 0.8918938715509556}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:35:54,278]\u001b[0m Trial 33 finished with value: 2.71529008266133 and parameters: {'max_depth': 6, 'learning_rate': 0.16417668568870278, 'n_estimators': 343, 'gamma': 0.4207670413999871, 'reg_alpha': 4.888164444292251, 'reg_lambda': 3.0093004930326677, 'colsample_bytree': 0.4086031839204882, 'subsample': 0.9875924100805102}. Best is trial 26 with value: 2.645893200633254.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:36:57,572]\u001b[0m Trial 34 finished with value: 2.6367168671009984 and parameters: {'max_depth': 6, 'learning_rate': 0.1611299070789704, 'n_estimators': 522, 'gamma': 0.5465424883228982, 'reg_alpha': 4.748305529814736, 'reg_lambda': 3.4069461876872893, 'colsample_bytree': 0.40203223545994377, 'subsample': 0.9947414739683336}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:38:08,229]\u001b[0m Trial 35 finished with value: 2.7819106788307137 and parameters: {'max_depth': 6, 'learning_rate': 0.16288304291386102, 'n_estimators': 557, 'gamma': 0.4394547233687599, 'reg_alpha': 4.576203592224544, 'reg_lambda': 4.18567658154856, 'colsample_bytree': 0.4034116000569597, 'subsample': 0.9219179238604689}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:39:04,034]\u001b[0m Trial 36 finished with value: 2.768713819552006 and parameters: {'max_depth': 8, 'learning_rate': 0.0878887365143561, 'n_estimators': 322, 'gamma': 0.23461340503634506, 'reg_alpha': 3.5634222930677195, 'reg_lambda': 3.553530371965561, 'colsample_bytree': 0.44160599567237824, 'subsample': 0.998016572197269}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:40:03,329]\u001b[0m Trial 37 finished with value: 2.8084496779078494 and parameters: {'max_depth': 6, 'learning_rate': 0.046873106136180356, 'n_estimators': 495, 'gamma': 0.5384874350806029, 'reg_alpha': 4.0119243432668785, 'reg_lambda': 3.1170347274672614, 'colsample_bytree': 0.40526750966484615, 'subsample': 0.9982601003519088}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:40:39,405]\u001b[0m Trial 38 finished with value: 2.799836609779389 and parameters: {'max_depth': 9, 'learning_rate': 0.12157374283777725, 'n_estimators': 190, 'gamma': 0.43847352317355176, 'reg_alpha': 4.584910188186716, 'reg_lambda': 3.662734175261453, 'colsample_bytree': 0.44148685224791134, 'subsample': 0.7969743364422199}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:42:32,171]\u001b[0m Trial 39 finished with value: 3.275629390683938 and parameters: {'max_depth': 5, 'learning_rate': 0.2617642277959996, 'n_estimators': 948, 'gamma': 0.11791305031181257, 'reg_alpha': 5.732716078715953, 'reg_lambda': 4.18908916068194, 'colsample_bytree': 0.48921649309689863, 'subsample': 0.8546311367538133}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:46:01,112]\u001b[0m Trial 40 finished with value: 3.087788548018911 and parameters: {'max_depth': 9, 'learning_rate': 0.15964791918719234, 'n_estimators': 717, 'gamma': 0.2502857397134819, 'reg_alpha': 3.9872640570866187, 'reg_lambda': 5.274943484677607, 'colsample_bytree': 0.6099442193112178, 'subsample': 0.9085878203394614}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:47:02,137]\u001b[0m Trial 41 finished with value: 2.8944054182929704 and parameters: {'max_depth': 8, 'learning_rate': 0.07334378477472776, 'n_estimators': 323, 'gamma': 0.2274805105169086, 'reg_alpha': 3.2978570446902093, 'reg_lambda': 3.404204635502607, 'colsample_bytree': 0.44410825786516306, 'subsample': 0.9839800209611976}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:48:10,747]\u001b[0m Trial 42 finished with value: 2.76846413320557 and parameters: {'max_depth': 8, 'learning_rate': 0.09799978307975948, 'n_estimators': 380, 'gamma': 0.3284500336847559, 'reg_alpha': 3.672856765453472, 'reg_lambda': 3.6417761127982384, 'colsample_bytree': 0.4370161067653121, 'subsample': 0.9513330812345817}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[32m[I 2023-06-20 12:49:20,918]\u001b[0m Trial 43 finished with value: 2.782677932262414 and parameters: {'max_depth': 7, 'learning_rate': 0.10717636836971697, 'n_estimators': 426, 'gamma': 0.33107843412183874, 'reg_alpha': 1.9527536362461235, 'reg_lambda': 4.798159113741566, 'colsample_bytree': 0.45608353237399135, 'subsample': 0.9427847189654018}. Best is trial 34 with value: 2.6367168671009984.\u001b[0m\n",
      "\u001b[33m[W 2023-06-20 12:49:33,257]\u001b[0m Trial 44 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_21111/2389013183.py\", line 16, in objective\n",
      "    tree_cv_res = run_cv(df=df_for_optuna, months_cv_split=tscv, model=xgb, cols_di=cols_di, verbose=0)\n",
      "  File \"/Users/igorchebuniaev/Documents/Обучение/Verious_repos/honest_home_assignment/src/utilities.py\", line 52, in run_cv\n",
      "    model.fit(X=train_df[cols_di['feats']], y=train_df[cols_di['target']])\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1025, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/Users/igorchebuniaev/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(tree_cv_res[\u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mregression_2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest parameters\u001b[39m\u001b[39m'\u001b[39m, study\u001b[39m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     14\u001b[0m xgb \u001b[39m=\u001b[39m XGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m     15\u001b[0m tscv \u001b[39m=\u001b[39m TimeSeriesSplit(test_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, max_train_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m tree_cv_res \u001b[39m=\u001b[39m run_cv(df\u001b[39m=\u001b[39;49mdf_for_optuna, months_cv_split\u001b[39m=\u001b[39;49mtscv, model\u001b[39m=\u001b[39;49mxgb, cols_di\u001b[39m=\u001b[39;49mcols_di, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(tree_cv_res[\u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Обучение/Verious_repos/honest_home_assignment/src/utilities.py:52\u001b[0m, in \u001b[0;36mrun_cv\u001b[0;34m(df, months_cv_split, model, cols_di, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m train_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mdate_block_num\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(train_months)]\n\u001b[1;32m     50\u001b[0m test_df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mdate_block_num\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(test_months)]\n\u001b[0;32m---> 52\u001b[0m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mtrain_df[cols_di[\u001b[39m'\u001b[39;49m\u001b[39mfeats\u001b[39;49m\u001b[39m'\u001b[39;49m]], y\u001b[39m=\u001b[39;49mtrain_df[cols_di[\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m     53\u001b[0m y_true \u001b[39m=\u001b[39m test_df[cols_di[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     54\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X\u001b[39m=\u001b[39mtest_df[cols_di[\u001b[39m'\u001b[39m\u001b[39mfeats\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_in_ksp--361CVkw/lib/python3.9/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_for_optuna = features_df.sample(int(features_df.shape[0]*.3))\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, .6),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 30, 1500),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 7.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 7.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0)\n",
    "    }\n",
    "    xgb = XGBRegressor(**params)\n",
    "    tscv = TimeSeriesSplit(test_size = 1, max_train_size=16)\n",
    "    tree_cv_res = run_cv(df=df_for_optuna, months_cv_split=tscv, model=xgb, cols_di=cols_di, verbose=0)\n",
    "    \n",
    "    return np.mean(tree_cv_res['rmse'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name='regression_2')\n",
    "study.optimize(objective, n_trials=500)\n",
    "print('Best parameters', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fast_good_params = {\n",
    "    'max_depth': 7, 'learning_rate': 0.19954706113792114, 'n_estimators': 362, \n",
    "    'gamma': 0.33508401169793434, 'reg_alpha': 6.092771756721105, 'reg_lambda': 2.8259388778880905, \n",
    "    'colsample_bytree': 0.46820045383805575, 'subsample': 0.8404918363404242\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train months: [13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28], size: 120,604\n",
      "  Test months: [29],   size: 6,911\n",
      "  NRMSE:  0.38\n",
      "  RMSE :  2.0\n",
      "\n",
      "Fold 1:\n",
      "  Train months: [14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], size: 119,612\n",
      "  Test months: [30],   size: 6,785\n",
      "  NRMSE:  0.43\n",
      "  RMSE :  1.9\n",
      "\n",
      "Fold 2:\n",
      "  Train months: [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30], size: 118,582\n",
      "  Test months: [31],   size: 5,784\n",
      "  NRMSE:  0.39\n",
      "  RMSE :  1.7\n",
      "\n",
      "Fold 3:\n",
      "  Train months: [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31], size: 117,026\n",
      "  Test months: [32],   size: 5,408\n",
      "  NRMSE:  0.66\n",
      "  RMSE :  3.2\n",
      "\n",
      "Fold 4:\n",
      "  Train months: [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32], size: 114,748\n",
      "  Test months: [33],   size: 5,568\n",
      "  NRMSE:  0.51\n",
      "  RMSE :  2.0\n",
      "\n",
      "\n",
      "------------------------------\n",
      "RMSE mean: 2.1\n",
      "NRMSE mean: 0.47\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(test_size = 1, max_train_size=16)\n",
    "# params = {'max_depth': 7, 'learning_rate': 0.005298559699829036, 'n_estimators': 300, 'gamma': 0.7744615794844047, \n",
    "#           'reg_alpha': 4.7427786108127865, 'reg_lambda': 3.9464923699087713, 'colsample_bytree': 0.5556916732637939}\n",
    "\n",
    "params = {'max_depth': 9, 'learning_rate': 0.00782545388868583, 'n_estimators': 704, 'gamma': 0.6242038569004144, \n",
    "'reg_alpha': 0.7315597123305507, 'reg_lambda': 3.9300592157864522, 'colsample_bytree': 0.419733637254625, \n",
    "'subsample': 0.737178584590776}\n",
    "\n",
    "tree_model = XGBRegressor(**params) #\n",
    "\n",
    "tree_cv_res = run_cv(df=features_df, months_cv_split=tscv, model=tree_model, cols_di=cols_di, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE mean: 2.3\n",
    "NRMSE mean: 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_in_ksp_new",
   "language": "python",
   "name": "rl_in_ksp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
